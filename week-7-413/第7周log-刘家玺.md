本周主要复习了机器学习中对数/交叉熵损失的推倒方法, 以下是我的笔记

### 对数损失

两个离散随机变量的分布的[相对熵](https://zh.wikipedia.org/wiki/%E7%9B%B8%E5%AF%B9%E7%86%B5)(Relative Entropy), 又称KL散度(Kullback–Leibler divergence), 是$D\_{KL}(p||q)=\sum\_{x} p(x)\ln \frac{q(x)}{p(i)}$, 其可以用于衡量两个概率分布之间的差异. 相对熵公式的前半部分是交叉熵(Cross Entropy), 即$H(p,q)=-\sum\_{x}p(x)\log q(x)$. 交叉熵可以用来衡量在给定的真实分布下, 使用非真实分布所指定的策略消除系统的不确定性所需要付出的努力的大小. 交叉熵损失的二分类版本就是逻辑回归损失. 交叉熵损失和逻辑回归损失都是对数损失(logloss).


==逻辑回归损失==
对于二分类问题, 设真实分布$y\in\\{0,1\\}$, 设$p={\rm Pr}(y=1)=1/(1+e^{-WX})$, 则每个样本的对数损失为: 

$$L(y,p)=-y\log(p)-(1-y)\log(1-p))$$ 

注意这里的表达式前必须有负号! 因为$y\_k\log p\_k$本身是负值. 所以交叉熵的值是正实数. 交叉熵损失越小, 说明模型拟合的越好.

**数学来源.** 对$y\_1,\dots,y\_N$的似然函数$\prod p(y\_i|x\_i,\theta)=\prod p^{y\_i}\cdot (1-p))^{(1-y\_i)}$$, 对此式取对数便得到上述逻辑回归损失.

==交叉熵损失==

对于$K$分类问题. 设某个样本的真实标签为$y=\[ y^1,\dots, y^K\]$(one-hot). 模型预测的分布为$p=\[p^1,\dots, p^K\]$(softmax函数变换后的), 则每个样本交叉熵损失为$L(y,p) = - \sum\limits\_{k=1}^{K}y\_k\log p\_k$, 至于整个训练集的交叉熵损失, 可以把每个样本的损失求和, 也可以求平均(实验中好像求平均比较好). 损失函数带有正则项的时候需要求平均, 否则求和求平均差不多.

dui数损失/交叉熵损失的由来:
- 是KL散度的马甲, KL散度用来衡量两个分布的差别
- 本质是通过MLE来推导出来的
- 形式比较符合"输出概率"的认知
- 见[这里](https://www.zhihu.com/question/65288314)
